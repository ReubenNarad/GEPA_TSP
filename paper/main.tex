\documentclass{article}

% Recommended packages from the ICML template (trimmed for availability)
\PassOptionsToPackage{hyphens}{url} % allow URL line breaks in references
\PassOptionsToPackage{hyphens}{url} % allow URL line breaks in references
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amssymb} % provides \mathbb and extra symbols

% Use the ICML 2025 style (camera-ready)
\usepackage[accepted]{icml2025}
\AtBeginDocument{\renewcommand{\ttdefault}{cmtt}} % avoid missing Courier tfm on this texlive

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{GEPA-TSP: Specializing Lin--Kernighan Heuristics}

\begin{document}

\twocolumn[
\icmltitle{GEPA-TSP: Specializing Lin--Kernighan Heuristics to Target Instance Distributions}

\begin{icmlauthorlist}
\icmlauthor{Reuben Narad}{uw}
\end{icmlauthorlist}

\icmlaffiliation{uw}{University of Washington}
\icmlcorrespondingauthor{Reuben Narad}{rnarad@uw.edu}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
    Large language models (LLMs) with reflective evolution have recently been used as search procedures over programs and prompts, offering a new way to adapt existing code to specific workloads. We study this idea in an operations research setting, tuning Concorde (a state-of-the-art exact TSP solver) by evolving the details of its pre-packaged Lin-Kernighan (LK) heuristic. Using GEPA, a reflective prompt evolution algorithm with separate actor and reflector LLMs, we build a sandbox that regenerates and benchmarks candidate heuristics. We target three 400-node TSP distributions of increasing structural complexity: uniform Euclidean, clustered Euclidean with in-cluster discounts, and a road-network derived from Seattle’s map. Across many runs, we observe a single robust win on Seattle instances, where GEPA found a change that improves solver time by 5\%. On other distributions, the same modification consistently regresses performance. Our results frame GEPA-style LLM search as a distribution-specific tuning method for existing heuristics, and highlight both the possibility and difficulty of beating carefully engineered baselines.
\end{abstract}

\section{Introduction}

Automated scientific discovery is an emerging area of interest, aiming to accelerate scientific research by automating parts of the work of an investigator. Instead of testing single experiments or code variations at a time, LLMs can run large batches of these tasks: testing many hypotheses, trying many variants of a codebase, or exploring many proof attempts in a loop. Many of these problems can be viewed as search over a large space of hypotheses, programs, or algorithmic components.

In such settings, the searching agent must keep past rollouts in context. If the agent does not know which experiments or variants worked previously, it will search in an inefficient, unguided way that ignores available feedback, making progress in a huge space effectively intractable. At the same time, fully remembering every experiment is impossible: the search space is too large to keep all history in context at once. This tension naturally motivates mechanisms that maintain a compact but useful long-term memory of past attempts.

Prompt learning and evolution provide one such mechanism. In approaches such as the DSPy framework \cite{DSPY}, the context of an agent as a major component of its performance, treating it as a learned, mutable object. Using a second "reflector" LLM, that context can be optimized to a given reward by being evolved over time based on the results of its earlier attempts. The Genetic Pareto (GEPA) \cite{GEPA!} algorithm decides which examples to keep in the reflector's context by maintaining a Pareto frontier of past candidates. Similar prompt-evolution ideas have begun to appear in automated discovery systems more broadly, including work like AlphaEvolve, where long-lived records of past trials guide future exploration.

Heuristics in operations research (OR) are a natural testbed for this paradigm. We study the traveling salesman problem (TSP) by taking Concorde and focusing on its Lin--Kernighan (LK) heuristic, which we allow an LLM to modify. We view the search process as testing different configurations or versions of this LK component. Concretely, we frame this as a task within the GEPA framework: a student model proposes rewrites of the LK heuristic, we recompile Concorde, run it on a benchmark of TSP instances solving to optimality, and use its runtime (relative to vanilla Concorde) and number of branch-and-bound search nodes as a reward signal to decide which examples to keep in the reflector model’s context. The reflector model then uses its in-context examples—actor prompts, the resulting heuristic code, and their measured performance—to propose changes to the actor’s prompt.

Our contributions are threefold. First, we present a reproducible sandbox for LK block injection in Concorde with full artifact logging, designed to provide the reflector with rich feedback for rewriting the actor’s prompt. Second, we curate a benchmark of non-Euclidean and Euclidean TSP instance distributions that expose differences between highly tuned baselines and under-optimized regimes. Third, we provide empirical evidence that a reflective LLM loop can discover a small improvement to Concorde on a structured travel-time distribution, while failing to improve and often degrading performance on classical Euclidean benchmarks. We release code, data, and all candidate LK variants to support further work on OR-driven automated science.


\section{Related Work}
\begin{itemize}
    \item \textbf{Classical LK and Concorde.} Foundational heuristics date to Lin--Kernighan's effective local search for TSP \cite{LinKernighan1973} and Helsgaun's LKH implementation \cite{Helsgaun2000LKH}; Concorde's implementation and engineering remain the reference standard \cite{Applegate2006TSP}.
    \item \textbf{Learning to optimize solvers.} A growing line of work learns heuristics or policies for combinatorial optimization \cite{Bello2017NCO,Kool2019AttentionRouting,Deudon2018TSPPolicyGrad,Costa2020Learn2Opt,Kerschke2018TSPSolverComplementarity}; our setting follows the same spirit but targets distribution-specific LK tweaks.
    \item \textbf{LLM-guided code evolution.} ReEvo frames LLMs as reflective hyper-heuristics that iteratively refine algorithms \cite{Ye2024ReEvo}; related work on reflective prompt evolution and LM pipeline optimization includes GEPA and DSPy \cite{Agrawal2025GEPA,Khattab2023DSPy}, as well as automated algorithm configuration and selection \cite{Hutter2009ParamILS,Hoos2011AutonomousSearchChapter,Kerschke2019AlgSelectionSurvey}; our GEPA loop similarly mutates and tests LK code but with a sandboxed, deterministic TSP pipeline.
\end{itemize}


\section{Method: GEPA for Lin--Kernighan}

\subsection{Background: How Concorde Works}

Concorde is an exact branch-and-bound solver that is heavily optimized for the TSP. It starts from the standard integer programming formulation of the TSP and relaxes it to a linear program. Because the LP is convex, it can be solved quickly. If the optimal LP solution happens to be integral, it is also an optimal solution to the original integer program.

When the LP solution is fractional, Concorde selects an edge whose LP value lies strictly between 0 and 1 and branches on it. One child node adds a constraint forcing this edge to be in the tour, and the other child node adds a constraint forcing it to be excluded. Recursively solving these subproblems yields a branch-and-bound tree. Alongside this tree search, Concorde runs a heuristic to maintain a best-known tour. Any branch whose LP relaxation cannot beat the heuristic tour length is pruned. The stronger this heuristic baseline, the more aggressively Concorde can prune, and the faster it converges to the optimal tour. Concorde ships with a carefully engineered Lin--Kernighan (LK) heuristic to provide this baseline.

\subsection{Turning the Heuristic into a GEPA Task}

In Concorde’s codebase, the LK heuristic is implemented as a well-defined C file, which we treat as a modular component. We build a workflow that replaces this LK code with a new heuristic block, recompiles Concorde, and then solves a fixed validation set of TSP instances. For each run, we log Concorde’s textual output, the total wall-clock time to solve the validation set, and the number of branch-and-bound nodes explored during the search. In GEPA, we use the total time and the node count as rewards.

For the purposes of GEPA, we view this reward as a function of the actor’s prompt. Let $\mathcal{P}$ denote the space of actor prompts and $\mathcal{H}$ the space of heuristic code blocks. The actor LLM is a (stochastic) mapping
\[
A : \mathcal{P} \to \mathcal{H}, \qquad h = A(p),
\]
which takes a prompt $p \in \mathcal{P}$ and produces a candidate heuristic $h \in \mathcal{H}$ (the LK replacement).
Given a fixed validation set $D_{\mathrm{val}}$ of TSP instances, the Concorde pipeline defines an evaluation map
\[
C : \mathcal{H} \times \mathcal{D} \to \mathbb{R}^2 \times \mathcal{T}, \qquad
C(h, D_{\mathrm{val}}) = (t, n, \tau),
\]
where $t$ is the total runtime on $D_{\mathrm{val}}$, $n$ is the total number of branch-and-bound nodes, and $\tau \in \mathcal{T}$ denotes the emitted trace (logs and solver output). The overall black-box objective that GEPA interacts with is the composition
\[
F : \mathcal{P} \to \mathbb{R}^2 \times \mathcal{T}, \qquad
F(p) = C(A(p), D_{\mathrm{val}}) = (t, n, \tau).
\]
We are interested in minimizing both $t$ and $n$ jointly: a candidate $(t', n')$ is preferred to $(t, n)$ when it Pareto-dominates it, i.e., $t' \le t$ and $n' \le n$ with at least one strict inequality. In practice, GEPA maintains a set of non-dominated examples in this two-dimensional objective space and uses them as in-context training data for the reflector. The actor LLM, code generation, compilation, and Concorde evaluation are internal details of this composite map; GEPA only sees prompts, their resulting $(t, n)$ pairs, and the logged artifacts used for reflection.



\begin{itemize}
    \item Non-Euclidean: structured\_seattle\_time (400 nodes) from OSM travel-time shortest paths (val/test splits of 20/50 instances).
    \item Euclidean: uniform\_val/test (400 nodes) and clustered\_val/test (400 nodes); metadata and seeds released.
    \item Other splits (toy20/200, tsplib\_random) maintained for smoke/regression; not central to main findings.
\end{itemize}

\section{Experimental Setup}

\subsection{Benchmarks and Data}

- Concorde is fast! For instances up to ~200 nodes, solution timne was so fast (<1s, 1 bbnode) that noise was the only differentiating factor.
- Thus, we focused on 400 node instances, checking that indeed Concorde typically used 4-6 BB nodes in the solving process
- For our experiments, we defined 3 types of TSP instance of increasing structure, with generators to sample new instances:
    
    - 2d Euclidian, Uniform sampling (basic TSP)
    - Clusters: For a given instance, we define the clusters (2-4, with centers and covariances drawn uniformly). Mostly Euclidian, but with a 50\% in-cluster travelling discount (to add structure)
    - 
- For a given GEPA candidate, the reward was 

\subsection{Gepa Details}

\begin{itemize}
    \item Models: student gpt-5-nano, reflector gpt-5-mini; reflection batch 2--3; 20 metric calls.
    \item Evaluation: per-instance repeats (3--5 on val), CPU affinity when available, timeouts off for reported runs; artifacts under \texttt{runs/}.
    \item Baseline: Concorde default LK rebuilt in the same sandbox; baseline repeats higher (5) for a stable reference.
    \item Variance: report per-instance averages; note that nontrivial gains require reproducible settings (affinity, binary hash).
\end{itemize}

\section{Results: TSP Adaptation}
\begin{sloppypar}
\begin{itemize}
    \item \textbf{Uniform (Euclidean, test 50 inst.):} baseline runtime $4.223$\,s / BB $3.84$ vs GEPA best $4.521$\,s / BB $3.96$ (\(+7.1\%\) runtime, \(+3.1\%\) BB). Plots: \url{out/gepa_uniform_n400_mean_std.png}; summaries: \url{runs/eval/eval/20251202T224329Z_uniform_test_baseline}, \url{runs/eval/eval/20251202T225646Z_uniform_test_gepa_iter40}.
    \item \textbf{Clustered (Euclidean, test 50 inst.):} baseline $4.405$\,s / BB $4.6$ vs GEPA $4.544$\,s / BB $5.2$ (\(+3.1\%\) runtime, \(+13.0\%\) BB). Plots: \url{out/gepa_clustered_20251129T211605Z.png}; summaries: \url{runs/eval/eval/20251202T230700Z_clustered_test_baseline}, \url{runs/eval/eval/20251202T231054Z_clustered_test_gepa_iter30}.
    \item \textbf{Seattle (travel-time, test 50 inst.):} baseline $3.958$\,s / BB $3.68$ vs GEPA $3.768$\,s / BB $3.55$ (\(-4.8\%\) runtime, \(-3.5\%\) BB); steady improvement in the rollout (see \url{out/gepa_structured_seattle_time_n400_time_smoothed_final.png}). Summaries: \url{runs/eval/eval/20251202T231521Z_seattle_time_test_baseline} (latest \url{runs/eval/eval/20251202T233939Z_seattle_time_test_baseline_latest}) and \url{runs/eval/eval/20251202T231850Z_seattle_time_test_gepa_iter31}.
    \item Overall: GEPA slows Euclidean cases relative to the tuned baseline, but yields a modest Seattle speedup while slightly reducing BB nodes, underscoring distribution-specific effects.
\end{itemize}
\end{sloppypar}

\section{Discussion}
\begin{itemize}
    \item GEPA excels when the baseline is not already tuned: non-Euclidean Seattle benefits; tuned Euclidean baselines do not.
    \item The learned tweak targets buffer/flush overhead; it may hurt when coherent batches are valuable (Euclidean).
    \item Reproducibility is essential: affinity, binary hashes, and artifact logging prevent confounding from build drift.
    \item Deployment: treat GEPA as per-distribution autotuning—run briefly on your workload, adopt the candidate if it beats your baseline.
\end{itemize}

\section{Conclusion}
\begin{itemize}
    \item GEPA can specialize LK for specific TSP distributions, yielding modest gains on non-Euclidean travel-time data while leaving tuned Euclidean baselines unchanged or slightly worse.
    \item Future work: multi-objective rewards, better diversity in proposals, and extensions beyond TSP.
    \item Release: code, data, and all candidate artifacts for reproducibility.
\end{itemize}

\bibliographystyle{icml2025}
\bibliography{references}

\end{document}
